---
title: "Final Exam"
author: "Matthew Davis"
date: "November 23,  2018"
output: html_document
---

## DATA Science 101 Final Exam
####Directions


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, comment = '')
```
Directions 
please use the .rmd file as a templet to upload to oaks a pdf, or word markdown document with the you answers to the uefollwoing questions.  All answers must be in your own words, not cut and pasted.  Working and groups as well as use of the internet for research is allowed, however each student must uniquely create their own answers.   Final exams are due to be uploaded no later than midnight December 4th.  


### linear regression

```{r}
data(mtcars)
summary(glm(mpg~carb, data = mtcars, family = 'gaussian'))$coefficients
```

+ According to this model, a car with zero carburators with be expected to have what mpg?


_______________________________________________________________________________________

+ For every additional carb, what is the expected increase or decrease in mpg?


_______________________________________________________________________________________

+ How do we know that number of carb is a significant predictor of mpg?


_______________________________________________________________________________________




+ Why is it impportant to split data into training and test set?



_____________________________________________

+ What is the difference between 10 fold cross validation and a random holdout when splitting data? Why would you use one vs the other?



_____________________________________________

+ How to decision tree use entropy to decide where to split variables?



_____________________________________________

Define the following parts of a decision tree: 

+ root:


+ node: 


+ leaf: 

#### Understanding error metrics
The following are error metrics of predictive models of a severely imbalanced data set:

* model1: training-set: accuracy 99.9%, auc 0.5, test-set: accuracy 99.9%, auc 0.5
* model2: training-set: accuracy 60% auc .6, test-set: accuracy 99.9%, auc 0.5
* model3: training_set: accuracy 95% auc .92, test_set: accuracy 75.9%, auc 0.63
* model4: training_set: accuracy 92% auc .85, test_set: accuracy 91%, auc 0.83

+ Which model is under fit?

+ Which model is over fit?

+ Which model learned a trivial solution?

#### regression metrics

Why would you choose mean squared (MSE) as a regression metric vs MAE (mean average error)?

A model has been trained on a training set with 10,000 training set rows and 2,500 test set rows

Given the following performance metrics Which is the decision tree model?

+ model1: 500 leaves, training-set: mse 1.1, r2_score .86 , test-set: mse 3.3 r2_score .75
+ model2: 250 leaves, training-set: mse 3.4, r2_score .8 , test-set: mse 3.3, r2_score .81
+ model3: 100 leaves, training-set: mse 4.0, r2_score .75 , test-set: mse 4.0 r2_score .75



______________________________________________

Explain Your Choice


______________________________________________

#### classificatio metrics

What is the difference between percision and recall in classifcation metrics?




______________________________________________


+ A data set has Null values in the independent variables?
Name two different ways to deal the Null values and why one approach may be used vs another.  



______________________________________________


#### CRISP DM with respect to final project
Describe in detail for each step of the CRISP-DM, the decisions and actions your team took to complete your final project. The CRisp-DM steps are as follows, please write a few sentences for each bullet point.

+ Business Understanding 
+ Data Understanding 
+ Data preparation/ Cleaning
+ Modeling 
+ Evaluation 
+ Deployment 








+ What was the biggest challenge in your final project, and how did you over come it?

























